{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "import pyspark.sql.functions as F # for SQL queries\n",
    "import pyspark.sql.types as T\n",
    "# !yarn application -kill application_1690360394514_7696"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "def getNaNs(data:pyspark.sql.DataFrame=None) -> None:\n",
    "    features = data.columns\n",
    "    n_observations = data.count()\n",
    "    \n",
    "    for feature in features:\n",
    "        n_wht_spcs = 25 - len(feature)\n",
    "        n_NaNs = data.filter((data[feature] == \"\") | data[feature].isNull() |F.isnan(data[feature])).count()\n",
    "        \n",
    "        n_wht_spcs_dgts = 25 - len(str(n_NaNs))\n",
    "        \n",
    "        print(\n",
    "            f'Number of NaN values:| {feature}', ' '*n_wht_spcs, f' | {n_NaNs}', \n",
    "            ' '*n_wht_spcs_dgts, f' | {(n_NaNs/n_observations)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark context creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/bd9/lib/python3.6/site-packages/ipykernel_launcher.py:45: UserWarning: Failed to initialize Spark session.\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 41, in <module>\n",
      "AttributeError: type object 'SparkSession' has no attribute '_create_shell_session'\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/bd9/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize `lab directory`\n",
    "lab_dir = '/labs'\n",
    "\n",
    "items_inf   = spark.read.csv(f'{lab_dir}/slaba03/laba03_items.csv', header=True, sep='\\t')\n",
    "test_smpl   = spark.read.csv(f'{lab_dir}/slaba03/laba03_test.csv', header=True)\n",
    "train_smpl  = spark.read.csv(f'{lab_dir}/slaba03/laba03_train.csv', header=True)\n",
    "vws_prgrmms = spark.read.csv(f'{lab_dir}/slaba03/laba03_views_programmes.csv', header=True)\n",
    "\n",
    "print(f'number of train_samples: {train_smpl.count()}; test_samples: {test_smpl.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### laba03_items.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ftrs = train_smpl.join(\n",
    "    other=items_inf.select('item_id', 'content_type', 'title', 'year', 'genres'),\n",
    "    on=['item_id'], how='left'\n",
    ")\n",
    "# getNaNs(data=train_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ftrs = train_ftrs.na.fill({'year': 'mode'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1.) deal with `genres` column\n",
    "# 1.1) lower values...\n",
    "train_ftrs = train_ftrs.withColumn('genres', F.lower(F.col('genres')))\n",
    "\n",
    "# 1.2) split value by sep=','\n",
    "train_ftrs = train_ftrs.withColumn(\"genres_list\", F.split(\"genres\", \",\\s*\"))\n",
    "\n",
    "# 1.3) define `is_cartoon` column\n",
    "train_ftrs = train_ftrs.withColumn('is_cartoon', F.when(F.col('genres').contains('мульт'), 1).otherwise(0))\n",
    "\n",
    "# 1.4) define `is_adult` column\n",
    "train_ftrs = train_ftrs.withColumn(\n",
    "    'is_adult', F.when((F.col('genres').contains('Эро') | F.col('genres').contains('взр')), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# 1.5) define `is_drama` column\n",
    "train_ftrs = train_ftrs.withColumn(\n",
    "    'is_drama', F.when((F.col('genres').contains('драм') | F.col('genres').contains('роман')), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# 1.6) define `is_russian` column\n",
    "train_ftrs = train_ftrs.withColumn(\n",
    "    'is_russian', \n",
    "    F.when(\n",
    "        (F.col('genres').contains('наши') | F.col('genres').contains('рус') | F.col('genres').contains('совет')), 1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# 2) define 'is_after_2000' column of content year\n",
    "train_ftrs = train_ftrs.withColumn('year', F.col('year').cast(T.IntegerType()))\n",
    "train_ftrs = train_ftrs.withColumn('is_after_2000', F.when(F.col('year') >= 2000, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) fraction of purchased content\n",
    "# 3.1) groupBy\n",
    "frac_purchased = train_ftrs.groupBy('item_id').agg(\n",
    "    F.expr('count(item_id)').alias('count'),\n",
    "    F.expr('sum(purchase)').alias('n_purchased'),\n",
    ")\n",
    "\n",
    "# 3.2) get fraction\n",
    "frac_purchased = frac_purchased.withColumn('frac_purchased_item', F.col('n_purchased') / F.col('count'))\n",
    "train_new_ftrs = train_ftrs.join(other=frac_purchased.select('item_id', 'frac_purchased_item'), on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) target encoding for 'is_cartoon', 'is_russian', 'is_after_2000'\n",
    "# 4.1) `is_cartoon`\n",
    "column = 'is_cartoon'\n",
    "mean_purchased_cartoon = train_new_ftrs.groupBy(column).agg(\n",
    "    F.expr('count(is_cartoon)').alias('count'),\n",
    "    F.expr('sum(purchase)').alias('n_purchased'),\n",
    ")\n",
    "\n",
    "mean_purchased_cartoon = mean_purchased_cartoon.withColumn(f'frac_purchased_{column}', F.col('n_purchased') / F.col('count'))\n",
    "train_new_ftrs = train_new_ftrs.join(\n",
    "    other=mean_purchased_cartoon.select('is_cartoon', f'frac_purchased_{column}'), on='is_cartoon'\n",
    ")\n",
    "\n",
    "# 4.2) `is_russian`\n",
    "column = 'is_russian'\n",
    "mean_purchased_cartoon = train_new_ftrs.groupBy(column).agg(\n",
    "    F.expr('count(is_russian)').alias('count'),\n",
    "    F.expr('sum(purchase)').alias('n_purchased'),\n",
    ")\n",
    "\n",
    "mean_purchased_cartoon = mean_purchased_cartoon.withColumn(f'frac_purchased_{column}', F.col('n_purchased') / F.col('count'))\n",
    "train_new_ftrs = train_new_ftrs.join(other=mean_purchased_cartoon.select(\n",
    "    'is_russian', f'frac_purchased_{column}'), on='is_russian'\n",
    ")\n",
    "\n",
    "# 4.3) `is_after_2000`\n",
    "column = 'is_after_2000'\n",
    "mean_purchased_cartoon = train_new_ftrs.groupBy(column).agg(\n",
    "    F.expr('count(is_after_2000)').alias('count'),\n",
    "    F.expr('sum(purchase)').alias('n_purchased'),\n",
    ")\n",
    "\n",
    "mean_purchased_cartoon = mean_purchased_cartoon.withColumn(f'frac_purchased_{column}', F.col('n_purchased') / F.col('count'))\n",
    "train_new_ftrs = train_new_ftrs.join(other=mean_purchased_cartoon.select(\n",
    "    'is_after_2000', f'frac_purchased_{column}'), on='is_after_2000'\n",
    ")\n",
    "\n",
    "# 4.4) `is_adult`\n",
    "column = 'is_adult'\n",
    "mean_purchased_cartoon = train_new_ftrs.groupBy(column).agg(\n",
    "    F.expr('count(is_adult)').alias('count'),\n",
    "    F.expr('sum(purchase)').alias('n_purchased'),\n",
    ")\n",
    "\n",
    "mean_purchased_cartoon = mean_purchased_cartoon.withColumn(f'frac_purchased_{column}', F.col('n_purchased') / F.col('count'))\n",
    "train_new_ftrs = train_new_ftrs.join(other=mean_purchased_cartoon.select(\n",
    "    'is_adult', f'frac_purchased_{column}'), on='is_adult'\n",
    ")\n",
    "\n",
    "# 4.5) `is_drama`\n",
    "column = 'is_drama'\n",
    "mean_purchased_cartoon = train_new_ftrs.groupBy(column).agg(\n",
    "    F.expr('count(is_drama)').alias('count'),\n",
    "    F.expr('sum(purchase)').alias('n_purchased'),\n",
    ")\n",
    "\n",
    "mean_purchased_cartoon = mean_purchased_cartoon.withColumn(f'frac_purchased_{column}', F.col('n_purchased') / F.col('count'))\n",
    "train_new_ftrs = train_new_ftrs.join(other=mean_purchased_cartoon.select(\n",
    "    'is_drama', f'frac_purchased_{column}'), on='is_drama'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### laba03_views_programmes.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) deal with 'ts_start' and `ts_end`\n",
    "# 2.1) convert to datetime format\n",
    "vws_prgrmms = vws_prgrmms.withColumn('timestamp_start', F.from_unixtime(\"ts_start\").cast(\"date\"))\n",
    "vws_prgrmms = vws_prgrmms.withColumn('timestamp_end', F.from_unixtime(\"ts_end\").cast(\"date\"))\n",
    "\n",
    "# 3) create `view_volume_difference` column\n",
    "vws_prgrmms = vws_prgrmms.withColumn('view_volume_difference_hours', (F.col('ts_end') - F.col('ts_start')) / 60 / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) make binary variable `item_type`\n",
    "vws_prgrmms = vws_prgrmms.withColumn('is_live', F.when(F.col('item_type') == 'live', 1).otherwise(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) group by `user_id` and get mean features `is_live`, `view_volume_difference_hours`\n",
    "usr_vws = vws_prgrmms.groupBy('user_id').agg(\n",
    "    F.sum('view_volume_difference_hours').alias('total_view'),\n",
    "    F.min('view_volume_difference_hours').alias('min_view'),\n",
    "    F.max('view_volume_difference_hours').alias('max_view'),\n",
    "    F.mean('view_volume_difference_hours').alias('avg_view'),\n",
    "    F.mean('is_live').alias('live_percentage')\n",
    ")\n",
    "train_new_ftrs = train_new_ftrs.join(other=usr_vws, on=['user_id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drp_clmns = ['genres', 'title']\n",
    "train_new_ftrs = train_new_ftrs.drop(*drp_clmns)\n",
    "\n",
    "train_new_ftrs = train_new_ftrs.withColumn('year', F.col('year').cast(T.IntegerType()))\n",
    "train_new_ftrs = train_new_ftrs.withColumn('purchase', F.col('purchase').cast(T.IntegerType()))\n",
    "train_new_ftrs = train_new_ftrs.withColumn('item_id', F.col('item_id').cast(T.IntegerType()))\n",
    "train_new_ftrs = train_new_ftrs.withColumn('content_type', F.col('content_type').cast(T.IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define fitting features\n",
    "fttng_ftrs = train_new_ftrs.drop(\n",
    "    'purchase', 'genres_list', 'user_id', 'item_id', 'is_cartoon', 'is_russian', 'is_after_2000', 'is_adult', 'is_drama'\n",
    ").columns\n",
    "# getNaNs(data=train_ftrs.select(*fttng_ftrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaNs with mean or mode values\n",
    "train_new_ftrs = train_new_ftrs.na.fill({\n",
    "    'total_view': 'mean', 'min_view': 'mode', 'max_view': 'mode', 'avg_view': 'mean',\n",
    "    'live_percentage': 'mean'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "vctr_assmblr = VectorAssembler(\n",
    "    inputCols=train_new_ftrs.drop(\n",
    "        'purchase', 'genres_list', 'user_id', 'item_id', 'is_cartoon', 'is_russian', 'is_after_2000', 'is_adult', 'is_drama'\n",
    "    ).columns, outputCol=\"features\"\n",
    ")\n",
    "train_data = vctr_assmblr.transform(train_new_ftrs.na.drop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(seed=42, featuresCol='features', labelCol=\"purchase\", maxIter=2)\n",
    "model = gbt.fit(train_data)\n",
    "train_prdctns = model.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='probability', labelCol='purchase', metricName='areaUnderROC')\n",
    "auc = evaluator.evaluate(train_prdctns)\n",
    "print('AUC: ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ftrs = test_smpl.join(\n",
    "    other=items_inf.select('item_id', 'content_type', 'title', 'year', 'genres'),\n",
    "    on=['item_id'], how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ftrs = train_ftrs.na.fill({'year': 'mode'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.) deal with `genres` column\n",
    "# 1.1) lower values...\n",
    "test_ftrs = test_ftrs.withColumn('genres', F.lower(F.col('genres')))\n",
    "\n",
    "# 1.2) split value by sep=','\n",
    "test_ftrs = test_ftrs.withColumn(\"genres_list\", F.split(\"genres\", \",\\s*\"))\n",
    "\n",
    "# 1.3) define `is_cartoon` column\n",
    "test_ftrs = test_ftrs.withColumn('is_cartoon', F.when(F.col('genres').contains('мульт'), 1).otherwise(0))\n",
    "\n",
    "# 1.4) define `is_adult` column\n",
    "test_ftrs = test_ftrs.withColumn(\n",
    "    'is_adult', F.when((F.col('genres').contains('Эро') | F.col('genres').contains('взр')), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# 1.5) define `is_drama` column\n",
    "test_ftrs = test_ftrs.withColumn(\n",
    "    'is_drama', F.when((F.col('genres').contains('драм') | F.col('genres').contains('роман')), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# 1.6) define `is_russian` column\n",
    "test_ftrs = test_ftrs.withColumn(\n",
    "    'is_russian', \n",
    "    F.when(\n",
    "        (F.col('genres').contains('наши') | F.col('genres').contains('рус') | F.col('genres').contains('совет')), 1\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# 2) define 'is_after_2000' column of content year\n",
    "test_ftrs = test_ftrs.withColumn('year', F.col('year').cast(T.IntegerType()))\n",
    "test_ftrs = test_ftrs.withColumn('is_after_2000', F.when(F.col('year') >= 2000, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) fraction of purchased content\n",
    "# 3.1) groupBy\n",
    "frac_purchased = train_ftrs.groupBy('item_id').agg(\n",
    "    F.expr('count(item_id)').alias('count'),\n",
    "    F.expr('sum(purchase)').alias('n_purchased'),\n",
    ")\n",
    "\n",
    "# 3.2) get fraction\n",
    "frac_purchased = frac_purchased.withColumn('frac_purchased_item', F.col('n_purchased') / F.col('count'))\n",
    "test_ftrs = test_ftrs.join(other=frac_purchased.select('item_id', 'frac_purchased_item'), on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) target encoding for 'is_cartoon', 'is_russian', 'is_after_2000'\n",
    "# 4.1) `is_cartoon`\n",
    "column = 'is_cartoon'\n",
    "mean_purchased_cartoon = train_new_ftrs.groupBy(column).agg(\n",
    "    F.expr('count(is_cartoon)').alias('count'),\n",
    "    F.expr('sum(purchase)').alias('n_purchased'),\n",
    ")\n",
    "\n",
    "mean_purchased_cartoon = mean_purchased_cartoon.withColumn(f'frac_purchased_{column}', F.col('n_purchased') / F.col('count'))\n",
    "test_ftrs = test_ftrs.join(\n",
    "    other=mean_purchased_cartoon.select('is_cartoon', f'frac_purchased_{column}'), on='is_cartoon'\n",
    ")\n",
    "\n",
    "# 4.2) `is_russian`\n",
    "column = 'is_russian'\n",
    "mean_purchased_cartoon = train_new_ftrs.groupBy(column).agg(\n",
    "    F.expr('count(is_russian)').alias('count'),\n",
    "    F.expr('sum(purchase)').alias('n_purchased'),\n",
    ")\n",
    "\n",
    "mean_purchased_cartoon = mean_purchased_cartoon.withColumn(f'frac_purchased_{column}', F.col('n_purchased') / F.col('count'))\n",
    "test_ftrs = test_ftrs.join(other=mean_purchased_cartoon.select(\n",
    "    'is_russian', f'frac_purchased_{column}'), on='is_russian'\n",
    ")\n",
    "\n",
    "# 4.3) `is_after_2000`\n",
    "column = 'is_after_2000'\n",
    "mean_purchased_cartoon = train_new_ftrs.groupBy(column).agg(\n",
    "    F.expr('count(is_after_2000)').alias('count'),\n",
    "    F.expr('sum(purchase)').alias('n_purchased'),\n",
    ")\n",
    "\n",
    "mean_purchased_cartoon = mean_purchased_cartoon.withColumn(f'frac_purchased_{column}', F.col('n_purchased') / F.col('count'))\n",
    "test_ftrs = test_ftrs.join(other=mean_purchased_cartoon.select(\n",
    "    'is_after_2000', f'frac_purchased_{column}'), on='is_after_2000'\n",
    ")\n",
    "\n",
    "# 4.4) `is_adult`\n",
    "column = 'is_adult'\n",
    "mean_purchased_cartoon = train_new_ftrs.groupBy(column).agg(\n",
    "    F.expr('count(is_adult)').alias('count'),\n",
    "    F.expr('sum(purchase)').alias('n_purchased'),\n",
    ")\n",
    "\n",
    "mean_purchased_cartoon = mean_purchased_cartoon.withColumn(f'frac_purchased_{column}', F.col('n_purchased') / F.col('count'))\n",
    "test_ftrs = test_ftrs.join(other=mean_purchased_cartoon.select(\n",
    "    'is_adult', f'frac_purchased_{column}'), on='is_adult'\n",
    ")\n",
    "\n",
    "# 4.5) `is_drama`\n",
    "column = 'is_drama'\n",
    "mean_purchased_cartoon = train_new_ftrs.groupBy(column).agg(\n",
    "    F.expr('count(is_drama)').alias('count'),\n",
    "    F.expr('sum(purchase)').alias('n_purchased'),\n",
    ")\n",
    "\n",
    "mean_purchased_cartoon = mean_purchased_cartoon.withColumn(f'frac_purchased_{column}', F.col('n_purchased') / F.col('count'))\n",
    "test_ftrs = test_ftrs.join(other=mean_purchased_cartoon.select(\n",
    "    'is_drama', f'frac_purchased_{column}'), on='is_drama'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ftrs = test_ftrs.join(other=usr_vws, on=['user_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drp_clmns = ['genres', 'title']\n",
    "test_ftrs = test_ftrs.drop(*drp_clmns)\n",
    "\n",
    "test_ftrs = test_ftrs.withColumn('year', F.col('year').cast(T.IntegerType()))\n",
    "test_ftrs = test_ftrs.withColumn('user_id', F.col('user_id').cast(T.IntegerType()))\n",
    "test_ftrs = test_ftrs.withColumn('item_id', F.col('item_id').cast(T.IntegerType()))\n",
    "test_ftrs = test_ftrs.withColumn('content_type', F.col('content_type').cast(T.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# replace NaNs with mean or mode values\n",
    "test_ftrs = test_ftrs.na.fill({\n",
    "    'total_view': 'mean', 'min_view': 'mode', 'max_view': 'mode', 'avg_view': 'mean',\n",
    "    'live_percentage': 'mean'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vctr_assmblr = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'is_drama', 'is_adult', 'is_after_2000', 'is_russian', 'is_cartoon', 'item_id', 'content_type', 'year',\n",
    "        'frac_purchased_item', 'frac_purchased_is_cartoon', 'frac_purchased_is_russian', 'frac_purchased_is_after_2000',\n",
    "        'frac_purchased_is_adult', 'frac_purchased_is_drama', 'total_view', 'min_view', 'max_view', 'avg_view',\n",
    "        'live_percentage',\n",
    "    ], outputCol=\"features\"\n",
    ")\n",
    "test_data = vctr_assmblr.transform(test_ftrs.drop('purchase'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prdctns = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prdctns = test_prdctns.withColumn('user_id', F.col('user_id').cast(T.IntegerType()))\n",
    "prbblts = test_prdctns.drop('features').select(\n",
    "    'user_id', 'item_id', 'probability'\n",
    ")\n",
    "\n",
    "firstelement = F.udf(lambda v: float(v[1]), T.FloatType())\n",
    "prbblts = prbblts.withColumn('buy_probability', firstelement('probability'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getNaNs(data=prbblts.drop('probability'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prbblts_pnds = prbblts.select(\n",
    "    'user_id', 'item_id', 'buy_probability'\n",
    ").orderBy(F.col('user_id').asc(), F.col('item_id').asc()).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prbblts_pnds = prbblts_pnds.rename(columns={'buy_probability': 'purchase'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prbblts_pnds.to_csv('./lab03.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark context stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
