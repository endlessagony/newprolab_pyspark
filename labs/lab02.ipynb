{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 2 pyspark-shell'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lab conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition structure id|language|course_name\n",
    "given_courses = [\n",
    "    [23126, u'en', u'Compass - powerful SASS library that makes your life easier'], \n",
    "    [21617, u'en', u'Preparing for the AP* Computer Science A Exam \\u2014 Part 2'], \n",
    "    [16627, u'es', u'Aprende Excel: Nivel Intermedio by Alfonso Rinsche'], \n",
    "    [11556, u'es', u'Aprendizaje Colaborativo by UNID Universidad Interamericana para el Desarrollo'], \n",
    "    [16704, u'ru', u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0430 Lazarus'], \n",
    "    [13702, u'ru', u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of observations: 28153\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|                 cat|                desc| id|lang|                name|      provider|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "|3/business_manage...|This course intro...|  4|  en|Accounting Cycle:...|Canvas Network|\n",
      "|              11/law|This online cours...|  5|  en|American Counter ...|Canvas Network|\n",
      "|5/computer_scienc...|This course is ta...|  6|  fr|Arithmétique: en ...|Canvas Network|\n",
      "|  14/social_sciences|We live in a digi...|  7|  en|Becoming a Dynami...|Canvas Network|\n",
      "|2/biology_life_sc...|This self-paced c...|  8|  en|           Bioethics|Canvas Network|\n",
      "+--------------------+--------------------+---+----+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "courses_path = '/labs/slaba02/DO_record_per_line.json'\n",
    "courses_info = spark.read.json(courses_path)\n",
    "print(f'number of observations: {courses_info.count()}')\n",
    "courses_info.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recommendation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re                                                  # for regular expressions\n",
    "from pyspark.ml.feature import StopWordsRemover            # for stopwords\n",
    "from pyspark.ml.feature import HashingTF                   # for hashing term frequences\n",
    "from pyspark.ml.feature import IDF                         # IDF\n",
    "from pyspark.ml import Pipeline                            # pipeline consturctor\n",
    "from pyspark.ml.feature import Tokenizer                   # for tokenization\n",
    "           \n",
    "from pyspark.sql.functions import pandas_udf               # for regular expressions\n",
    "from pyspark.sql.functions import regexp_replace           # for regular expressions\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.window import Window                      # for selecting top-10\n",
    "from pyspark.sql.functions import col, row_number          ##############################\n",
    "  \n",
    "from pyspark.ml.feature import Normalizer                  # for L2 norm computation\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start with combine all information at one column `full_info`\n",
    "# .1) separate categories by '/'\n",
    "courses_info = courses_info.withColumn('cat_cleaned', regexp_replace('cat', '/', ' '))\n",
    "courses_info = courses_info.withColumn('cat_cleaned', regexp_replace('cat_cleaned', '_', ' '))\n",
    "courses_info = courses_info.withColumn('cat_cleaned', regexp_replace('cat_cleaned', '\\|', ' '))\n",
    "\n",
    "courses_info = courses_info.withColumn(\n",
    "    'full_info', F.concat_ws(' ', F.col('cat_cleaned'), F.col('desc'), F.col('name'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) remove stopwords and punctuation from description...\n",
    "# 1.1) remove punсtuation firstly\n",
    "regex_punctuations = '[!@\"“’«»#$%&\\'()*+,—/:;<=>?^_`{|}~\\[\\]]'\n",
    "courses_info = courses_info.withColumn('cleaned_desc', regexp_replace('full_info', regex_punctuations, ''))\n",
    "\n",
    "# 1.2) lower string values\n",
    "courses_info = courses_info.withColumn('cleaned_desc', F.lower(F.col('cleaned_desc')))\n",
    "\n",
    "# 1.3) removing number either\n",
    "courses_info = courses_info.withColumn('cleaned_desc', regexp_replace('cleaned_desc', r'[0-9]', ''))\n",
    "\n",
    "# 1.4) delete stopWords\n",
    "# 1.4.1) tokenize description\n",
    "tokenizer = Tokenizer(inputCol='cleaned_desc', outputCol='tokenz')\n",
    "tokenized = tokenizer.transform(courses_info)\n",
    "\n",
    "# 1.4.2) fit StopWordsRemover\n",
    "remover = StopWordsRemover(inputCol='tokenz', outputCol='filtered_tokenz')\n",
    "filtered_tokenized = remover.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate metric\n",
    "# 1) calculate term frequency (TF)\n",
    "hashingTF = HashingTF(inputCol='filtered_tokenz', outputCol='features')\n",
    "hashingTF.setNumFeatures(10000)\n",
    "TF = hashingTF.transform(filtered_tokenized)\n",
    "\n",
    "# 2) calculate IDF\n",
    "IDF_ = IDF(inputCol='features', outputCol='IDF').fit(TF)\n",
    "TF_IDF = IDF_.transform(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3) get all candidates for recommendations\n",
    "columns = ['id', 'lang', 'IDF', 'name']\n",
    "course_candidates_ids = [values[0] for values in given_courses]\n",
    "course_candidates = TF_IDF.filter(F.col('id').isin(course_candidates_ids)).select(columns)\n",
    "course_candidates_spark = course_candidates.select('id').distinct().rdd.flatMap(lambda value: value).collect()\n",
    "assert set(course_candidates_ids) - set(course_candidates_spark) == set([])\n",
    "assert set(course_candidates_spark) - set(course_candidates_ids) == set([])\n",
    "\n",
    "# 3.1) join to them, all other courses on candidates' language\n",
    "course_recommended = TF_IDF.filter(\n",
    "    ~F.col('id').isin(course_candidates_ids)\n",
    ").select([F.col(column).alias(column + '_recommended') for column in columns])\n",
    "course_candidates = course_candidates.crossJoin(course_recommended)\n",
    "course_candidates = course_candidates.filter(F.col('lang') == F.col('lang_recommended'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2) cosine similarity calculation\n",
    "@udf(returnType=T.DoubleType())\n",
    "def cosine_similarity(v1, v2):\n",
    "    return float(v1.dot(v2) / (v1.norm(2) * v2.norm(2)))\n",
    "\n",
    "course_candidates = course_candidates.withColumn(\n",
    "    'cosine_similarity', cosine_similarity(F.col('IDF'), F.col('IDF_recommended'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 4) result preparation\n",
    "# 4.1) check the same languages\n",
    "lab02 = course_candidates.filter(F.col('lang') == F.col('lang_recommended'))\n",
    "\n",
    "# 4.2) select only top-10\n",
    "window_ = Window.partitionBy(\"id\").orderBy(\n",
    "    F.asc('id'), F.desc('cosine_similarity'), F.asc('name_recommended'), F.asc('id_recommended')\n",
    ")\n",
    "lab02 = lab02.withColumn(\"row\", row_number().over(window_))\n",
    "lab02 = lab02.filter(col(\"row\") <= 10).orderBy(\n",
    "    F.asc('id'), F.desc('cosine_similarity'), F.asc('name_recommended'), F.asc('id_recommended')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3) compress into .json\n",
    "result_json = {\n",
    "    row['id']: row['collect_set(id_recommended)'] for row in \\\n",
    "    lab02.select('id', 'id_recommended').groupby(\"id\").agg(F.collect_set(\"id_recommended\")).collect()\n",
    "} \n",
    "\n",
    "# save json\n",
    "with open('lab02.json', 'w') as file:\n",
    "    json.dump(result_json, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{23126: [25782, 13348, 14760, 2724, 24419, 13665, 13782, 23756, 15909, 20638],\n",
       " 16627: [13529, 13021, 12247, 11431, 25010, 12863, 12660, 11575, 5687, 17964],\n",
       " 13702: [795, 948, 8082, 956, 1052, 1110, 853, 1216, 8313, 864],\n",
       " 16704: [1376, 1236, 18331, 8154, 1426, 1229, 1164, 1365, 20105, 8203],\n",
       " 11556: [23357, 9289, 10447, 16488, 19330, 22710, 13461, 10384, 7833, 468],\n",
       " 21617: [21616, 21703, 21506, 21608, 21492, 21609, 21624, 21675, 21508, 21700]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark context stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
