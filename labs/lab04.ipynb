{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkContext creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--num-executors 5 --executor-memory 4g --executor-cores 2 --driver-memory 8g pyspark-shell'\n",
    "\n",
    "\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import Row\n",
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF\n",
    "import pyspark.sql.types \n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import json\n",
    "\n",
    "conf = SparkConf()\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"Streaming_lab\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   3 hdfs hdfs  655090069 2022-01-06 18:46 /labs/slaba04/gender_age_dataset.txt\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /labs/slaba04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data path and shema\n",
    "path = '/labs/slaba04/gender_age_dataset.txt'\n",
    "schema = StructType([\n",
    "    StructField('gender', StringType()),\n",
    "    StructField('age', StringType()),\n",
    "    StructField('uid', StringType()),\n",
    "    StructField('user_json', StringType()) \n",
    "])\n",
    "\n",
    "# read data and get rid of missing value in gender and age\n",
    "train_data = spark.read.csv(\n",
    "    path, header = True, schema = schema, sep = '\\t'\n",
    ").where((F.col('gender') != '-') & (F.col('age') != '-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we have `user_json` feature and we should also define structure for it\n",
    "user_json_schema = StructType([\n",
    "    StructField('visits', ArrayType(\n",
    "        StructType([\n",
    "            StructField('url', StringType(), True),\n",
    "            StructField('timestamp', LongType(), True)\n",
    "        ])\n",
    "    ))\n",
    "])\n",
    "train_data = train_data.withColumn('parsed_user', F.from_json('user_json', schema=user_json_schema))\n",
    "\n",
    "# one more thing - explode `visit` feature\n",
    "train_data_exploded = train_data.withColumn(\"visit\", F.explode(F.col(\"parsed_user.visits\")))\n",
    "train_data_exploded = train_data_exploded.select('*', F.col('visit.url'), F.col('visit.timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain(url:str=None):\n",
    "    '''\n",
    "    Function extracts domains from url\n",
    "        Args:\n",
    "            url:str\n",
    "                Current URL\n",
    "        Returns:\n",
    "            domain:str\n",
    "                Domain of the particular URL\n",
    "    '''\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.netloc\n",
    "\n",
    "domain_udf = F.udf(extract_domain, StringType())\n",
    "cleared_urls = train_data_exploded.withColumn(\"clean_url\", domain_udf(F.col(\"url\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cleared_urls.select('age', 'gender', 'uid', 'clean_url')\n",
    "\n",
    "# collect all visited links by user\n",
    "visited_links = features.groupBy('uid').agg(F.collect_list('clean_url').alias('all_links'))\n",
    "\n",
    "# define `unique` datas only and join with visited links\n",
    "unique_data = features.select('uid', 'age', 'gender').dropDuplicates(['uid'])\n",
    "features_n_target = visited_links.join(other=unique_data, on=['uid'], how='inner').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create HashingTF for all visited links\n",
    "hashingTF = HashingTF(inputCol='all_links', outputCol='rawFeatures', numFeatures=30000)\n",
    "features_n_target_preprocessed = hashingTF.transform(features_n_target).drop('all_links')\n",
    "\n",
    "# define and apply IDF\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "idfModel = idf.fit(features_n_target_preprocessed)\n",
    "features_n_target_preprocessed = idfModel.transform(features_n_target_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define StringIndexer for `gender` column \n",
    "label_indexer_gender = StringIndexer(inputCol=\"gender\", outputCol=\"label_gender\").fit(features_n_target_preprocessed)\n",
    "features_n_target_preprocessed = label_indexer_gender.transform(features_n_target_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define StringIndexer for `gender` column \n",
    "label_indexer_age = StringIndexer(inputCol=\"age\", outputCol=\"label_age\").fit(features_n_target_preprocessed)\n",
    "features_n_target_preprocessed = label_indexer_age.transform(features_n_target_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deffine train-test split\n",
    "fractions_age = features_n_target_preprocessed.select(\"age\").distinct().withColumn(\"fraction\", F.lit(0.75)).rdd.collectAsMap()\n",
    "train_age = features_n_target_preprocessed.stat.sampleBy(\"age\", fractions_age, seed=2002)\n",
    "test_age = features_n_target_preprocessed.join(train_age, on=[\"uid\"], how=\"left_anti\")\n",
    "\n",
    "fractions_gender = features_n_target_preprocessed.select(\"gender\").distinct().withColumn(\n",
    "    \"fraction\", F.lit(0.75)\n",
    ").rdd.collectAsMap()\n",
    "train_gender = features_n_target_preprocessed.stat.sampleBy(\"gender\", fractions_gender, seed=2002)\n",
    "test_gender = features_n_target_preprocessed.join(train_gender, on=[\"uid\"], how=\"left_anti\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Age` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.433396\n"
     ]
    }
   ],
   "source": [
    "randomForestAge = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=\"label_age\", featuresCol=\"features\", seed=2002)\n",
    "model_randomForestAge = randomForestAge.fit(train_age)\n",
    "\n",
    "predictions_age = model_randomForestAge.transform(test_age)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_age\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions_age)\n",
    "print(\"Accuracy = %g\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Gender` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.525694\n"
     ]
    }
   ],
   "source": [
    "randomForestGender = RandomForestClassifier(numTrees=3, maxDepth=2, labelCol=\"label_gender\", featuresCol=\"features\", seed=2002)\n",
    "model_randomForestGender = randomForestGender.fit(train_gender)\n",
    "\n",
    "predictions_gender = model_randomForestGender.transform(test_gender)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label_gender\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions_gender)\n",
    "print(\"Accuracy = %g\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_BOOTSTRAP_SERVER = 'spark-master-1.newprolab.com:6667'\n",
    "KAFKA_INPUT_SERVER = 'input_nikita.ermishov'\n",
    "KAFKA_OUTPUT_TOPIC = 'nikita.ermishov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_console_sink(df):\n",
    "    '''\n",
    "    Function that allow to show df\n",
    "    '''\n",
    "    \n",
    "    return df.writeStream.format(\"console\").trigger(processingTime=\"5 seconds\").option(\n",
    "        \"truncate\", \"false\"\n",
    "    ).option(\"numRows\", \"20\")\n",
    "\n",
    "kafka_read_sdf = spark.readStream.format('kafka').option(\n",
    "    'kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVER\n",
    ").option('subscribe', KAFKA_INPUT_SERVER).option('startingOffsets', 'earliest').option('failOnDataLoss', 'False').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka_read_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schemas\n",
    "event_schema = StructType([\n",
    "    StructField('uid', StringType(), True),\n",
    "    StructField('visits', StringType(), True) \n",
    "])\n",
    "\n",
    "visits_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField('url', StringType(), True),\n",
    "        StructField('timestamp', LongType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# and streaming dataframe\n",
    "clean_sdf = kafka_read_sdf.select(F.col('value').cast('string').alias('value')).select(\n",
    "    F.from_json(F.col('value'), event_schema).alias('event')\n",
    ").select('event.uid', F.from_json(F.col('event.visits'), visits_schema).alias('visits'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_explode = clean_sdf.withColumn(\"url\", F.explode(F.col(\"visits.url\"))).select('uid', 'url')\n",
    "sdf_example = sdf_explode.withColumn(\"clean_url\", domain_udf(F.col(\"url\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_collect = sdf_example.groupBy('uid').agg(F.collect_list('clean_url').alias('all_links'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define all transforms as it was before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol='all_links', outputCol='rawFeatures', numFeatures=30000)\n",
    "sdf_features = hashingTF.transform(sdf_collect).drop('all_links')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_features = sdf_features.withColumn('features', F.col('rawFeatures'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test_age = model_randomForestAge.transform(sdf_features)\n",
    "prediction_test_gender = model_randomForestGender.transform(sdf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_age = IndexToString(inputCol = 'prediction', outputCol = 'age', labels = label_indexer_age.labels)\n",
    "prediction_test_age = convert_age.transform(prediction_test_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_gender = IndexToString(inputCol = 'prediction', outputCol = 'gender', labels = label_indexer_gender.labels)\n",
    "prediction_test_gender = convert_gender.transform(prediction_test_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_prediction = prediction_test_gender.join(prediction_test_age, 'uid', 'inner').select('uid', 'gender', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_kafka_out = (\n",
    "    sdf_prediction\n",
    "    .select(F.to_json(F.struct(*sdf_prediction.columns)).alias('value'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.DataStreamWriter at 0x7f9290eac2e8>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    sdf_kafka_out\n",
    "    .writeStream\n",
    "    .format('kafka')\n",
    "    .outputMode('append')\n",
    "    .option('kafka.bootstrap.servers', KAFKA_BOOTSTRAP_SERVER)\n",
    "    .option('topic', KAFKA_OUTPUT_TOPIC)\n",
    "    #.save()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sq.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
